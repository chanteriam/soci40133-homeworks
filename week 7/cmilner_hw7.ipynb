{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 - Prediction & Causal Inference\n",
    "\n",
    "Last week, we explored (supervised) text classification, where we train a model to learn associations between text and some classification or value connected with it (e.g., what distinguishes a winning argument before the Supreme Court; can we extend our judgment regarding what documents are relevant to my thesis project to all of Google News; etc.) Classification often uses a representative sample of text about which we want to make inferences and then we use machine learning to learn \"true\" assignments and classify the rest.\n",
    "\n",
    "This week, we explore two different types of inferences to out-of-sample populations. _Prediction_ involves our reasoned expectation regarding an unobserved state of the world, given the world in which we live and on which we have trained our prediction algorithm. Often this prediction is about the future world. We don't expect the U.S. Congress to talk about the identical things today and tomorrow, but today should contain some useful information. by contrast _causal inference_ poses the related by distinct challenge of our reasoned expectations regarding an unobserved state of the world IF we intervene in some way. In other words, what does the intervention cause, and how can we predict it to change the world. Causality has a deeply contested history in social science and philosophy, but it usually involves an \"if,\" a difference between two counterfactual worlds, one where an event occurs and one where it doesn't.\n",
    "\n",
    "Causal questions in text analysis may place the text in one or more of many positions we explore below: as cause, effect, confounder, mediator (or moderator), or collider. For example, assuming that everything spoken can be transcribed into text, saying something mean might hurt someone's feelings (text as cause). Doing something mean might cause someone to say something angry (text as effect). Apologizing might change the influence of doing something mean (text as mediator/moderator). A compliment might obscure the effect of doing something mean (text as confounder). And yelling something audaciously mean might yield a loud, emotional response, which both influence the likelihood that the interaction was recorded and subjected to analysis (text as collider). As you can see, in a single conversation, text can play all of these roles. Why do we care about cause and effect with text? Because while words appear to exert power in the world, which words spoken under what circumstances by whom? Causal analysis attempts to get at the question, if _X_ was written or spoken, _Y_ would happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installs if necessary\n",
    "%pip install -U git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "%pip install statsmodels\n",
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lucem_illud\n",
    "\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# statsmodels is a popular Python statistics package\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics.api as smg\n",
    "from statsmodels.stats.mediation import Mediation\n",
    "\n",
    "# Pipelines to add text-based quantiative variables for regressions\n",
    "from transformers import pipeline\n",
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# We have a lot of features, so let's set Pandas to show all of them.\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape data\n",
    "url = \"http://nldslab.soe.ucsc.edu/iac/iac_v1.1.zip\"\n",
    "req = requests.get(url)\n",
    "\n",
    "# save data\n",
    "data_directory = \"/Users/shaymilner/Library/Mobile Documents/com~apple~CloudDocs/Harris/Winter24/Content Analysis/assignments/soci40133-homeworks/data\"\n",
    "filepath = os.path.join(data_directory, url.split(\"/\")[-1])\n",
    "os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
    "\n",
    "with open(filepath, \"wb\") as output_file:\n",
    "    output_file.write(req.content)\n",
    "print(\"Downloaded file: \" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip data\n",
    "with zipfile.ZipFile(filepath) as z:\n",
    "    with z.open(\n",
    "        \"iac_v1.1/data/fourforums/annotations/mechanical_turk/qr_averages.csv\"\n",
    "    ) as f:\n",
    "        qr = pd.read_csv(f)\n",
    "\n",
    "    with z.open(\n",
    "        \"iac_v1.1/data/fourforums/annotations/mechanical_turk/qr_meta.csv\"\n",
    "    ) as f:\n",
    "        md = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pairs\n",
    "pairs = qr.merge(md, how='inner', on='key')\n",
    "pairs = pairs[~pairs.quote_post_id.isnull() & ~pairs.response_post_id.isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>discussion_id_x</th>\n",
       "      <th>agree-disagree</th>\n",
       "      <th>agreement</th>\n",
       "      <th>agreement_unsure</th>\n",
       "      <th>attack</th>\n",
       "      <th>attack_unsure</th>\n",
       "      <th>defeater-undercutter</th>\n",
       "      <th>defeater-undercutter_unsure</th>\n",
       "      <th>fact-feeling</th>\n",
       "      <th>fact-feeling_unsure</th>\n",
       "      <th>negotiate-attack</th>\n",
       "      <th>negotiate-attack_unsure</th>\n",
       "      <th>nicenasty</th>\n",
       "      <th>nicenasty_unsure</th>\n",
       "      <th>personal-audience</th>\n",
       "      <th>personal-audience_unsure</th>\n",
       "      <th>questioning-asserting</th>\n",
       "      <th>questioning-asserting_unsure</th>\n",
       "      <th>sarcasm</th>\n",
       "      <th>sarcasm_unsure</th>\n",
       "      <th>discussion_id_y</th>\n",
       "      <th>response_post_id</th>\n",
       "      <th>quote_post_id</th>\n",
       "      <th>term</th>\n",
       "      <th>task1 num annot</th>\n",
       "      <th>task2 num annot</th>\n",
       "      <th>task2 num disagree</th>\n",
       "      <th>quote</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(731, 1)</td>\n",
       "      <td>6032</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-4.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6032</td>\n",
       "      <td>149609</td>\n",
       "      <td>149552.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>I remember looking at the classic evolutionary...</td>\n",
       "      <td>Why do you find it necessary to fit observatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(660, 3)</td>\n",
       "      <td>10217</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10217</td>\n",
       "      <td>277697</td>\n",
       "      <td>277459.0</td>\n",
       "      <td>yes</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>So they (pro-life peeps) say abortion is murde...</td>\n",
       "      <td>Yes, you are missing something. How come age d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(114, 5)</td>\n",
       "      <td>3462</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>-1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-4.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-1.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3462</td>\n",
       "      <td>76012</td>\n",
       "      <td>75976.0</td>\n",
       "      <td>No terms in first 10</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>'If the solar system was brought about by an a...</td>\n",
       "      <td>C.S.Lewis believes things on faith, yet we are...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        key  discussion_id_x  agree-disagree  agreement  agreement_unsure  \\\n",
       "0  (731, 1)             6032        0.333333  -1.333333          0.333333   \n",
       "1  (660, 3)            10217        0.600000   0.285714          0.000000   \n",
       "2  (114, 5)             3462        0.600000  -1.500000          0.000000   \n",
       "\n",
       "     attack  attack_unsure  defeater-undercutter  defeater-undercutter_unsure  \\\n",
       "0  0.333333            0.0                   0.5                          0.0   \n",
       "1  0.714286            0.0                  -2.5                          0.0   \n",
       "2  1.333333            0.0                   1.0                          0.0   \n",
       "\n",
       "   fact-feeling  fact-feeling_unsure  negotiate-attack  \\\n",
       "0      0.333333             0.333333               3.0   \n",
       "1      1.000000             0.000000              -2.0   \n",
       "2      1.500000             0.000000              -1.5   \n",
       "\n",
       "   negotiate-attack_unsure  nicenasty  nicenasty_unsure  personal-audience  \\\n",
       "0                     0.25   0.666667          0.166667              -2.25   \n",
       "1                     0.00   1.142857          0.000000              -1.50   \n",
       "2                     0.00   2.166667          0.000000              -4.00   \n",
       "\n",
       "   personal-audience_unsure  questioning-asserting  \\\n",
       "0                      0.25                  -4.25   \n",
       "1                      0.00                   0.50   \n",
       "2                      0.00                  -1.50   \n",
       "\n",
       "   questioning-asserting_unsure   sarcasm  sarcasm_unsure  discussion_id_y  \\\n",
       "0                           0.0  0.200000        0.166667             6032   \n",
       "1                           0.0  0.142857        0.000000            10217   \n",
       "2                           0.0  0.000000        0.000000             3462   \n",
       "\n",
       "   response_post_id  quote_post_id                  term  task1 num annot  \\\n",
       "0            149609       149552.0                   NaN                6   \n",
       "1            277697       277459.0                   yes                7   \n",
       "2             76012        75976.0  No terms in first 10                6   \n",
       "\n",
       "   task2 num annot  task2 num disagree  \\\n",
       "0                6                   4   \n",
       "1                5                   2   \n",
       "2                5                   2   \n",
       "\n",
       "                                               quote  \\\n",
       "0  I remember looking at the classic evolutionary...   \n",
       "1  So they (pro-life peeps) say abortion is murde...   \n",
       "2  'If the solar system was brought about by an a...   \n",
       "\n",
       "                                            response  \n",
       "0  Why do you find it necessary to fit observatio...  \n",
       "1  Yes, you are missing something. How come age d...  \n",
       "2  C.S.Lewis believes things on faith, yet we are...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get triples\n",
    "# Self-merge where the 'response' matches another 'quote' in the DataFrame\n",
    "triples = pairs.merge(pairs,left_on='response',right_on='quote',how='inner',suffixes=('_r1','_r2'))\n",
    "\n",
    "# Rename and reorder columns\n",
    "triples = triples.rename(columns={'quote_r1':'quote', 'quote_r2':'response1', 'response_r2':'response2'})\n",
    "triples = triples.drop(columns=['response_r1'])\n",
    "front_columns = [\n",
    "                 'quote','response1','response2','attack_r1','fact-feeling_r1','nicenasty_r1','sarcasm_r1',\n",
    "                 'agreement_r2'\n",
    "                ]\n",
    "triples = triples.dropna(subset=front_columns)\n",
    "triples = triples[front_columns].join(triples.drop(columns=front_columns))\n",
    "\n",
    "# add length variable\n",
    "triples['length_r1'] = triples['response1'].apply(lambda x: len(x))\n",
    "triples['length_r2'] = triples['response2'].apply(lambda x: len(x))\n",
    "triples['length_q'] = triples['quote'].apply(lambda x: len(x))\n",
    "\n",
    "# add sentiment\n",
    "triples['sentiment_r1'] = triples['response1'].apply(lambda x: sentiment(x[:512])[0]['score'])\n",
    "triples['sentiment_r2'] = triples['response2'].apply(lambda x: sentiment(x[:512])[0]['score'])\n",
    "triples['sentiment_q'] = triples['quote'].apply(lambda x: sentiment(x[:512])[0]['score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quote</th>\n",
       "      <th>response1</th>\n",
       "      <th>response2</th>\n",
       "      <th>attack_r1</th>\n",
       "      <th>fact-feeling_r1</th>\n",
       "      <th>nicenasty_r1</th>\n",
       "      <th>sarcasm_r1</th>\n",
       "      <th>agreement_r2</th>\n",
       "      <th>key_r1</th>\n",
       "      <th>discussion_id_x_r1</th>\n",
       "      <th>agree-disagree_r1</th>\n",
       "      <th>agreement_r1</th>\n",
       "      <th>agreement_unsure_r1</th>\n",
       "      <th>attack_unsure_r1</th>\n",
       "      <th>defeater-undercutter_r1</th>\n",
       "      <th>defeater-undercutter_unsure_r1</th>\n",
       "      <th>fact-feeling_unsure_r1</th>\n",
       "      <th>negotiate-attack_r1</th>\n",
       "      <th>negotiate-attack_unsure_r1</th>\n",
       "      <th>nicenasty_unsure_r1</th>\n",
       "      <th>personal-audience_r1</th>\n",
       "      <th>personal-audience_unsure_r1</th>\n",
       "      <th>questioning-asserting_r1</th>\n",
       "      <th>questioning-asserting_unsure_r1</th>\n",
       "      <th>sarcasm_unsure_r1</th>\n",
       "      <th>discussion_id_y_r1</th>\n",
       "      <th>response_post_id_r1</th>\n",
       "      <th>quote_post_id_r1</th>\n",
       "      <th>term_r1</th>\n",
       "      <th>task1 num annot_r1</th>\n",
       "      <th>task2 num annot_r1</th>\n",
       "      <th>task2 num disagree_r1</th>\n",
       "      <th>key_r2</th>\n",
       "      <th>discussion_id_x_r2</th>\n",
       "      <th>agree-disagree_r2</th>\n",
       "      <th>agreement_unsure_r2</th>\n",
       "      <th>attack_r2</th>\n",
       "      <th>attack_unsure_r2</th>\n",
       "      <th>defeater-undercutter_r2</th>\n",
       "      <th>defeater-undercutter_unsure_r2</th>\n",
       "      <th>fact-feeling_r2</th>\n",
       "      <th>fact-feeling_unsure_r2</th>\n",
       "      <th>negotiate-attack_r2</th>\n",
       "      <th>negotiate-attack_unsure_r2</th>\n",
       "      <th>nicenasty_r2</th>\n",
       "      <th>nicenasty_unsure_r2</th>\n",
       "      <th>personal-audience_r2</th>\n",
       "      <th>personal-audience_unsure_r2</th>\n",
       "      <th>questioning-asserting_r2</th>\n",
       "      <th>questioning-asserting_unsure_r2</th>\n",
       "      <th>sarcasm_r2</th>\n",
       "      <th>sarcasm_unsure_r2</th>\n",
       "      <th>discussion_id_y_r2</th>\n",
       "      <th>response_post_id_r2</th>\n",
       "      <th>quote_post_id_r2</th>\n",
       "      <th>term_r2</th>\n",
       "      <th>task1 num annot_r2</th>\n",
       "      <th>task2 num annot_r2</th>\n",
       "      <th>task2 num disagree_r2</th>\n",
       "      <th>length_r1</th>\n",
       "      <th>length_r2</th>\n",
       "      <th>length_q</th>\n",
       "      <th>sentiment_r1</th>\n",
       "      <th>sentiment_r2</th>\n",
       "      <th>sentiment_q</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I remember looking at the classic evolutionary...</td>\n",
       "      <td>Why do you find it necessary to fit observatio...</td>\n",
       "      <td>Evolution has no goals, it is merely a beautif...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.833333</td>\n",
       "      <td>(731, 1)</td>\n",
       "      <td>6032</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-2.250000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>-4.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6032</td>\n",
       "      <td>149609</td>\n",
       "      <td>149552.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>(610, 2)</td>\n",
       "      <td>6032</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>-4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6032</td>\n",
       "      <td>149673</td>\n",
       "      <td>149609.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>263</td>\n",
       "      <td>117</td>\n",
       "      <td>265</td>\n",
       "      <td>0.997491</td>\n",
       "      <td>0.972950</td>\n",
       "      <td>0.998637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What is the fun in that?</td>\n",
       "      <td>Seriously? Well, I come here hoping for someth...</td>\n",
       "      <td>nah, I was just poking fun because I can! Pers...</td>\n",
       "      <td>-0.600000</td>\n",
       "      <td>-2.200000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.166667</td>\n",
       "      <td>(697, 2)</td>\n",
       "      <td>5205</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>-2.400000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5205</td>\n",
       "      <td>122800</td>\n",
       "      <td>122780.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>(1267, 0)</td>\n",
       "      <td>5205</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.333333</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>5205</td>\n",
       "      <td>123129</td>\n",
       "      <td>122800.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>356</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>0.990721</td>\n",
       "      <td>0.994051</td>\n",
       "      <td>0.999512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>First off, the scientific method goes:\\n \\n 1)...</td>\n",
       "      <td>You guys know me. Always happy to correct anyo...</td>\n",
       "      <td>Ah, thanks for the correction, although there ...</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.400000</td>\n",
       "      <td>(9, 0)</td>\n",
       "      <td>9449</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>-3.666667</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>9449</td>\n",
       "      <td>247240</td>\n",
       "      <td>247225.0</td>\n",
       "      <td>you</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>(1393, 1)</td>\n",
       "      <td>9449</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>9449</td>\n",
       "      <td>247243</td>\n",
       "      <td>247240.0</td>\n",
       "      <td>No terms in first 10</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1544</td>\n",
       "      <td>198</td>\n",
       "      <td>169</td>\n",
       "      <td>0.998007</td>\n",
       "      <td>0.843735</td>\n",
       "      <td>0.996069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               quote  \\\n",
       "0  I remember looking at the classic evolutionary...   \n",
       "1                           What is the fun in that?   \n",
       "2  First off, the scientific method goes:\\n \\n 1)...   \n",
       "\n",
       "                                           response1  \\\n",
       "0  Why do you find it necessary to fit observatio...   \n",
       "1  Seriously? Well, I come here hoping for someth...   \n",
       "2  You guys know me. Always happy to correct anyo...   \n",
       "\n",
       "                                           response2  attack_r1  \\\n",
       "0  Evolution has no goals, it is merely a beautif...   0.333333   \n",
       "1  nah, I was just poking fun because I can! Pers...  -0.600000   \n",
       "2  Ah, thanks for the correction, although there ...   2.400000   \n",
       "\n",
       "   fact-feeling_r1  nicenasty_r1  sarcasm_r1  agreement_r2    key_r1  \\\n",
       "0         0.333333      0.666667         0.2     -2.833333  (731, 1)   \n",
       "1        -2.200000      0.000000         0.0     -2.166667  (697, 2)   \n",
       "2         2.800000      2.200000         0.0     -0.400000    (9, 0)   \n",
       "\n",
       "   discussion_id_x_r1  agree-disagree_r1  agreement_r1  agreement_unsure_r1  \\\n",
       "0                6032           0.333333     -1.333333             0.333333   \n",
       "1                5205           0.833333     -2.400000             0.000000   \n",
       "2                9449           0.400000      0.600000             0.200000   \n",
       "\n",
       "   attack_unsure_r1  defeater-undercutter_r1  defeater-undercutter_unsure_r1  \\\n",
       "0               0.0                 0.500000                             0.0   \n",
       "1               0.0                -5.000000                             0.0   \n",
       "2               0.2                -2.666667                             0.0   \n",
       "\n",
       "   fact-feeling_unsure_r1  negotiate-attack_r1  negotiate-attack_unsure_r1  \\\n",
       "0                0.333333             3.000000                        0.25   \n",
       "1                0.000000             2.000000                        0.00   \n",
       "2                0.200000            -3.666667                        0.00   \n",
       "\n",
       "   nicenasty_unsure_r1  personal-audience_r1  personal-audience_unsure_r1  \\\n",
       "0             0.166667             -2.250000                         0.25   \n",
       "1             0.000000              0.000000                         0.00   \n",
       "2             0.200000              0.333333                         0.00   \n",
       "\n",
       "   questioning-asserting_r1  questioning-asserting_unsure_r1  \\\n",
       "0                 -4.250000                              0.0   \n",
       "1                 -2.000000                              0.0   \n",
       "2                  3.666667                              0.0   \n",
       "\n",
       "   sarcasm_unsure_r1  discussion_id_y_r1  response_post_id_r1  \\\n",
       "0           0.166667                6032               149609   \n",
       "1           0.000000                5205               122800   \n",
       "2           0.200000                9449               247240   \n",
       "\n",
       "   quote_post_id_r1 term_r1  task1 num annot_r1  task2 num annot_r1  \\\n",
       "0          149552.0     NaN                   6                   6   \n",
       "1          122780.0     NaN                   5                   6   \n",
       "2          247225.0     you                   5                   5   \n",
       "\n",
       "   task2 num disagree_r1     key_r2  discussion_id_x_r2  agree-disagree_r2  \\\n",
       "0                      4   (610, 2)                6032                0.6   \n",
       "1                      1  (1267, 0)                5205                0.6   \n",
       "2                      3  (1393, 1)                9449                1.0   \n",
       "\n",
       "   agreement_unsure_r2  attack_r2  attack_unsure_r2  defeater-undercutter_r2  \\\n",
       "0             0.166667   0.333333          0.166667                     -3.5   \n",
       "1             0.333333   0.833333          0.166667                     -1.5   \n",
       "2             0.200000   0.800000          0.200000                      NaN   \n",
       "\n",
       "   defeater-undercutter_unsure_r2  fact-feeling_r2  fact-feeling_unsure_r2  \\\n",
       "0                             0.0         1.333333                0.166667   \n",
       "1                             0.0        -1.333333                0.500000   \n",
       "2                             NaN         1.200000                0.200000   \n",
       "\n",
       "   negotiate-attack_r2  negotiate-attack_unsure_r2  nicenasty_r2  \\\n",
       "0                  3.5                         0.0           0.5   \n",
       "1                  2.0                         0.0           0.5   \n",
       "2                  NaN                         NaN           1.0   \n",
       "\n",
       "   nicenasty_unsure_r2  personal-audience_r2  personal-audience_unsure_r2  \\\n",
       "0             0.333333                  -4.0                          0.0   \n",
       "1             0.166667                  -3.0                          0.0   \n",
       "2             0.400000                   NaN                          NaN   \n",
       "\n",
       "   questioning-asserting_r2  questioning-asserting_unsure_r2  sarcasm_r2  \\\n",
       "0                       1.5                              0.0         0.0   \n",
       "1                      -1.5                              0.0         0.2   \n",
       "2                       NaN                              NaN         0.0   \n",
       "\n",
       "   sarcasm_unsure_r2  discussion_id_y_r2  response_post_id_r2  \\\n",
       "0           0.166667                6032               149673   \n",
       "1           0.166667                5205               123129   \n",
       "2           0.400000                9449               247243   \n",
       "\n",
       "   quote_post_id_r2               term_r2  task1 num annot_r2  \\\n",
       "0          149609.0                   NaN                   6   \n",
       "1          122800.0                   NaN                   6   \n",
       "2          247240.0  No terms in first 10                   5   \n",
       "\n",
       "   task2 num annot_r2  task2 num disagree_r2  length_r1  length_r2  length_q  \\\n",
       "0                   5                      2        263        117       265   \n",
       "1                   5                      2        356        152        24   \n",
       "2                   7                      0       1544        198       169   \n",
       "\n",
       "   sentiment_r1  sentiment_r2  sentiment_q  \n",
       "0      0.997491      0.972950     0.998637  \n",
       "1      0.990721      0.994051     0.999512  \n",
       "2      0.998007      0.843735     0.996069  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Exercise 1*\n",
    "\n",
    "**Describe 2 separate predictions relevant to your project and associated texts, which involve predicting text that has not been observed based on patterns that have. Then, in a single, short paragraph, describe a research design through which you could use textual features and the tools of classification and regression to evaluate these predictions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two potential predictions:\n",
    "1. Predict the effect of SCOTUS opinions on subsequent congressional legislation.\n",
    "2. Predict the effect of SCOTUS opinions on media coverage of abortion.\n",
    "\n",
    "SCOTUS opinions often include novel arguments for or against a given social issue. In the 1973 *Roe v. Wade* case, SCOTUS argued for the constitutional right to abortion based on the concept of privacy, an argument that had not been made before. Conversely, the 2022 *Dobbs v Jackson* opinion made the case against a constitutional right to abortion based on the lack of constitutional precedent for the practice. It is reasonable to assume that these novel arguments will influence the way congressional legislation following the opinions frames pro- or anti-abortion policies. Additionally, the way that the media covers abortion access across the US will begin to adopt these arguments, and even more, the outcome of arguments could cause a rise in counter-argumentative media articles (e.g., after *Roe*, an increased presence of anti-abortion media coverage). To assess the first prediction, we could use SCOTUS opinions as bookmarks in time, and assses the language used in the subsequent congressional legislation up to the next major SCOTUS opinion. With this, we can extract and weight key terms (nouns, adverbs, etc.) in the scotus case, and use these key terms to predict the type of congressional legislation to follow based on their own key terms and phrases. For the second article, we can adopt a similar model using media articles instead of congressional legislation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Exercise 2*</font>\n",
    "\n",
    "**Propose a simple causal model in your data, or a different causal model in the annotated Internet Arguments Corpus (e.g., a different treatment, a different outcome), and test it using a linear or logistic regression model. If you are using social media data for your final project, we encourage you to classify or annotate a sample of that data (either compuationally or with human annotators) and examine the effect of texts on replies to that text (e.g., Reddit posts on Reddit comments, Tweets on Twitter replies, YouTube video transcripts on YouTube comments or ratings). You do not need to make a graph of the causal model, but please make it clear (e.g., \"X affects Y, and C affects both X and Y.\").**   \n",
    "\n",
    "**Also consider using the [ConvoKit datasets](https://convokit.cornell.edu/documentation/datasets.html)! Anytime there is conversation, there is an opportunity to explore the effects of early parts of the conversation on later parts. We will explore this further in Week 8 on Text Generation and Conversation.**\n",
    "\n",
    "***Stretch*** *(not required): Propose a more robust identification strategy using either matching, difference in difference, regression discontinuity, or an instrumental variable. Each of these methods usually gives you a more precise identification of the causal effect than a unconditional regression. Scott Cunningham's [Causal Inference: The Mixtape](https://mixtape.scunning.com/) is a free textbook on these topics, and all have good YouTube video explanations.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fact v. Feeling\n",
    "Test whether a person's lean toward using fact or feeling based arguments in their text affected the lean of their respondent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:           agreement_r2   R-squared:                       0.013\n",
      "Model:                            OLS   Adj. R-squared:                  0.012\n",
      "Method:                 Least Squares   F-statistic:                     17.95\n",
      "Date:                Wed, 21 Feb 2024   Prob (F-statistic):           2.43e-05\n",
      "Time:                        12:19:59   Log-Likelihood:                -2572.2\n",
      "No. Observations:                1340   AIC:                             5148.\n",
      "Df Residuals:                    1338   BIC:                             5159.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==========================================================================================\n",
      "                             coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------------------\n",
      "const                     -1.4440      0.058    -24.736      0.000      -1.559      -1.330\n",
      "fact-feeling_unsure_r1     1.3837      0.327      4.236      0.000       0.743       2.024\n",
      "==============================================================================\n",
      "Omnibus:                       84.869   Durbin-Watson:                   1.851\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              100.554\n",
      "Skew:                           0.631   Prob(JB):                     1.46e-22\n",
      "Kurtosis:                       3.459   Cond. No.                         7.34\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "# Run OLS regression\n",
    "ex_y = triples[\"agreement_r2\"]\n",
    "ex_x_cols = [\"fact-feeling_unsure_r1\"]\n",
    "ex_x = sm.add_constant(triples[ex_x_cols])\n",
    "\n",
    "ex_lm = sm.OLS(ex_y, ex_x).fit()\n",
    "print(ex_lm.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "In the above OLS linear regression, we tested if the initial responder's lean toward a factual or feeling-based response had a causal effect on the second response's agreement measurement. Based on the data above, it has a significant effect (p <.001) on whether the second response agrees. Though it does have a significant effect, it does not account for much of the variation in agreement_r2, based on the Adjusted R-squared value (0.012), with the value of fact-feeling_unsure_r1 only accounting for 1.2% of the variation in agreement_r2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *Exercise 3*\n",
    "\n",
    "**Propose a measure you could generate to fill in or improve upon the simple causal model you proposed above and how you would split the data (e.g., a % of your main data, a separate-but-informative dataset). You do not have to produce the measure.**\n",
    "    \n",
    "***Stretch*** *(not required): Produce the measure and integrate it into your statistical analysis. This could be a great approach for your final project!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the example to improve upon our causal model. To improve the model, we could incorporate length as a variable in our model, using `len(r1.split())`. We can add this to the first response to validate the assumption that feeling statements are often shorter than factual statements. We think the length of the first response would improve how much the variation in the data was explained by our model ($R^2$). Given that there multiple pairs that do not belong to the triples collection, we could use that as our split. We could then test our proposed model on the pair split, and validate it on the triples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "**Propose a mediation model related to the simple causal model you proposed above (ideally on the dataset you're using for your final project). If you have measures for each variable in the model, run the analysis: You can just copy the \"Mediation analysis\" cell above and replace with your variables. If you do not have measures, do not run the analysis, but be clear as to the effect(s) you would like to estimate and the research design you would use to test them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moderating Fact/Feeling\n",
    "We can't use the fact-feeling lean since the original quotes weren't coded. However, we can estimate the effect of text sentiment on agreement. Specifically, is there a causal chain of sentiment through a conversation as a result of sentiment and comment length?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Estimate</th>\n",
       "      <th>Lower CI bound</th>\n",
       "      <th>Upper CI bound</th>\n",
       "      <th>P-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ACME (control)</th>\n",
       "      <td>-8.414639e-04</td>\n",
       "      <td>-1.181939e-01</td>\n",
       "      <td>1.144867e-01</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACME (treated)</th>\n",
       "      <td>-8.414639e-04</td>\n",
       "      <td>-1.181939e-01</td>\n",
       "      <td>1.144867e-01</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADE (control)</th>\n",
       "      <td>-3.464066e-17</td>\n",
       "      <td>-1.442277e-16</td>\n",
       "      <td>-4.557420e-18</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADE (treated)</th>\n",
       "      <td>-3.479096e-17</td>\n",
       "      <td>-1.456135e-16</td>\n",
       "      <td>-4.429756e-18</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total effect</th>\n",
       "      <td>-8.414639e-04</td>\n",
       "      <td>-1.181939e-01</td>\n",
       "      <td>1.144867e-01</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prop. mediated (control)</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prop. mediated (treated)</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ACME (average)</th>\n",
       "      <td>-8.414639e-04</td>\n",
       "      <td>-1.181939e-01</td>\n",
       "      <td>1.144867e-01</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ADE (average)</th>\n",
       "      <td>-3.471581e-17</td>\n",
       "      <td>-1.455391e-16</td>\n",
       "      <td>-4.363343e-18</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prop. mediated (average)</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Estimate  Lower CI bound  Upper CI bound  \\\n",
       "ACME (control)           -8.414639e-04   -1.181939e-01    1.144867e-01   \n",
       "ACME (treated)           -8.414639e-04   -1.181939e-01    1.144867e-01   \n",
       "ADE (control)            -3.464066e-17   -1.442277e-16   -4.557420e-18   \n",
       "ADE (treated)            -3.479096e-17   -1.456135e-16   -4.429756e-18   \n",
       "Total effect             -8.414639e-04   -1.181939e-01    1.144867e-01   \n",
       "Prop. mediated (control)  1.000000e+00    1.000000e+00    1.000000e+00   \n",
       "Prop. mediated (treated)  1.000000e+00    1.000000e+00    1.000000e+00   \n",
       "ACME (average)           -8.414639e-04   -1.181939e-01    1.144867e-01   \n",
       "ADE (average)            -3.471581e-17   -1.455391e-16   -4.363343e-18   \n",
       "Prop. mediated (average)  1.000000e+00    1.000000e+00    1.000000e+00   \n",
       "\n",
       "                          P-value  \n",
       "ACME (control)              0.968  \n",
       "ACME (treated)              0.968  \n",
       "ADE (control)               0.000  \n",
       "ADE (treated)               0.000  \n",
       "Total effect                0.968  \n",
       "Prop. mediated (control)    0.000  \n",
       "Prop. mediated (treated)    0.000  \n",
       "ACME (average)              0.968  \n",
       "ADE (average)               0.000  \n",
       "Prop. mediated (average)    0.000  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mediation analysis\n",
    "y = triples['agreement_r1']\n",
    "X_cols = ['sentiment_q','length_q']\n",
    "X = sm.add_constant(triples[X_cols])\n",
    "mediator_model = sm.OLS(y,X)\n",
    "\n",
    "# For the second step of the mediation model, we can add in other predictors.\n",
    "y = triples['agreement_r1']\n",
    "X_cols = ['sentiment_q','length_q', 'agreement_r1']\n",
    "X = sm.add_constant(triples[X_cols])\n",
    "outcome_model = sm.OLS(y,X)\n",
    "\n",
    "med = Mediation(outcome_model=outcome_model, mediator_model=mediator_model,\n",
    "                exposure='length_q', mediator='agreement_r1').fit()\n",
    "med.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "Similar to the example, the Average Causal Mediated Effect isn't significantly distinct from randomness, so there doesn't seem to be a causal link through responses. However, the ADE is significant, indicating a significant effect of quote sentiment on response agreement.\n",
    "\n",
    "The lack of a causal link could be because respondents don't necessarily respond to previous comments; instead, each respondent could be independently responding to the original quote itself. We would need a data structure of responses to responses to quotes (like on Twitter/X, where you can respond to other responses) in order to see if such a mediated effect exists.\n",
    "\n",
    "Current data structure:<br>\n",
    "```plain text\n",
    "original quote\n",
    "|__ response 1\n",
    "|__ response 2\n",
    "```\n",
    "\n",
    "Ideal data structure for mediation analysis:<br>\n",
    "```plain text\n",
    "original quote\n",
    "|__ response 1\n",
    "|  |__ response 1.1\n",
    "|  |__ response 1.2\n",
    "|__ response 2\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "**Pick one other paper on causal inference with text from the [\"Papers about Causal Inference and Language\" GitHub repository](https://github.com/causaltext/causal-text-papers). Write at least three sentences summarizing the paper and its logic of design in your own words.**\n",
    "    \n",
    "***Stretch*** *(not required): Skim a few more papers. The causal world is your textual oyster!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Article\n",
    "Veitch, Victor, Dhanya Sridhar, and David M. Blei. 2020. \"Adapting Text Embeddings for Causal Inference.\" *Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence (UAI)*, vol. 124. [https://arxiv.org/pdf/1905.12741.pdf](https://arxiv.org/pdf/1905.12741.pdf). \n",
    "\n",
    "### Summary\n",
    "This research article introduces a method to understand the causal impact of certain features in text documents, such as the inclusion of a theorem in a paper or mentioning an author's gender in a post, on outcomes like paper acceptance or post popularity. It tackles the problem of texts being too complex and high-dimensional for traditional causal inference methods by developing \"causally sufficient embeddings\". These are low-dimensional representations of documents that maintain crucial information for identifying causal effects while disregarding irrelevant details. The method combines language modeling techniques with supervised dimensionality reduction, focusing only on text aspects predictive of both the intervention (like adding a theorem) and the outcome (such as paper acceptance). The approach is validated through semi-synthetic datasets, showing improvements in causal estimation, and the article discusses potential future improvements and the challenges in assessing the assumptions behind these black box models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
