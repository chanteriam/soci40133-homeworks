{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 - Exploring Semantic Spaces (Word Embeddings)\n",
    "This week, we build on last week's topic modeling techniques by taking a text corpus we have developed, specifying an underlying number of dimensions, and training a model with a neural network auto-encoder (one of Google's word2vec  algorithms) that best describes corpus words in their local linguistic contexts, and exploring their locations in the resulting space to learn about the discursive culture that produced them.\n",
    "\n",
    "This is our third document representation we have learned: First, we used word counts. Second, we used LDA topic models built around term coocurrence in the same document (i.e., a \"bag of words\"). Third, documents here are represented as densely indexed locations in dimensions, so that distances between those documents (and words) contain more information, though they require the full vector of dimension loadings (rather than just a few selected topic loadings) to describe. We will explore these spaces to understand complex, semantic relationships between words, index documents with descriptive words, identify the likelihood that a given document would have been produced by a given vector model, and explore how semantic categories can help us understand the cultures that produced them.\n",
    "\n",
    "Note that most modern natural language processing (NLP) research, at least in computer science, uses word embeddings. This is the foundation of most state-of-the-art models.\n",
    "\n",
    "Also note that the code in this Notebook can take many minutes or even hours to run. This is the case for most NLP research these days, and it's a good opportunity to start thinking about how to manage high-compute workloads, such as running code on small samples to test it, loading datafiles in [chunks](https://stackoverflow.com/a/25962187), or [multiprocessing](https://en.wikipedia.org/wiki/Multiprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"red\">*Pitch Your Project*</font>\n",
    "\n",
    "<font color=\"red\">In the three cells immediately following, describe **WHAT** you are planning to analyze for your final project (i.e., texts, contexts and the social game, world and actors you intend to learn about through your analysis) (<200 words), **WHY** you are going to do it (i.e., why would theory and/or the average person benefit from knowing the results of your investigation) (<200 words), and **HOW** you plan to investigate it (i.e., what are the approaches and operations you plan to perform, in sequence, to yield this insight) (<400 words).</font>\n",
    "\n",
    "### ***What?***\n",
    "For our project, Michael Plunkett and I will be analyzing congressional and supreme court abortion legislation. Particularly, for the congressional legislation, we will be analyzing all legislation since 1973 until 2024. This legislation was pulled from the [congress.gov legislation search](https://www.congress.gov/advanced-search/legislation?congressGroup%5B%5D=0&congresses%5B%5D=118&congresses%5B%5D=117&congresses%5B%5D=116&congresses%5B%5D=115&congresses%5B%5D=114&congresses%5B%5D=113&congresses%5B%5D=112&congresses%5B%5D=111&congresses%5B%5D=110&congresses%5B%5D=109&congresses%5B%5D=108&congresses%5B%5D=107&congresses%5B%5D=106&congresses%5B%5D=105&congresses%5B%5D=104&congresses%5B%5D=103&congresses%5B%5D=102&congresses%5B%5D=101&congresses%5B%5D=100&congresses%5B%5D=99&congresses%5B%5D=98&congresses%5B%5D=97&congresses%5B%5D=96&congresses%5B%5D=95&congresses%5B%5D=94&congresses%5B%5D=93&legislationNumbers=&restrictionType=field&restrictionFields%5B%5D=allBillTitles&restrictionFields%5B%5D=summary&summaryField=billSummary&enterTerms=%22reproductive+health+care%22%2C+%22reproduction%22%2C+%22abortion%22&legislationTypes%5B%5D=hr&legislationTypes%5B%5D=hjres&legislationTypes%5B%5D=s&legislationTypes%5B%5D=sjres&public=true&private=true&chamber=all&actionTerms=&legislativeActionWordVariants=true&dateOfActionOperator=equal&dateOfActionStartDate=&dateOfActionEndDate=&dateOfActionIsOptions=yesterday&dateOfActionToggle=multi&legislativeAction=Any&sponsorState=One&member=&sponsorTypes%5B%5D=sponsor&sponsorTypeBool=OR&dateOfSponsorshipOperator=equal&dateOfSponsorshipStartDate=&dateOfSponsorshipEndDate=&dateOfSponsorshipIsOptions=yesterday&committeeActivity%5B%5D=0&committeeActivity%5B%5D=3&committeeActivity%5B%5D=11&committeeActivity%5B%5D=12&committeeActivity%5B%5D=4&committeeActivity%5B%5D=2&committeeActivity%5B%5D=5&committeeActivity%5B%5D=9&satellite=null&search=&submitted=Submitted), where we filtered for any legislation within this time period that could have become bills and included the following keywords in the bill text or summary: 'abortion', 'reproduction', or 'reproductive health care'. For the SCOTUS abortion legislation, we targetted SCOTUS decisions outlined on supreme.justia.com, which [provides a list of abortion-relavant SCOTUS decision](https://supreme.justia.com/cases-by-topic/abortion-reproductive-rights/) from 1965-2022. Through this analysis, we plan to uncover the ways that the legislative abortion discourse has changed overtime, and the promonent congressional bills that occur throughout subsequent legislations. We will supplement this analysis by legislation in its political history, by taking note of the political affiliations of congressmembers, SCOTUS justices, and the presidency.\n",
    "\n",
    "### ***Why?***\n",
    "This analysis in general will give us an understanding of how abortion discourse has changed over time, and specifically what arguments were used in the passing of the 1973 Roe v. Wade decision--and with it, assertion of the constitutional right to abortion--and also in its eventual reversal in the 2022 Dobbs v. Jackson decision. From this, we will be able to uncover what political mechanisms were at play that enabled this regression in legislation, and how such large cases went on to influence congressional legislation that followed. The average person will in general be able to understand how sensitive practices are discussed in the political sphere and what aspects in particular are targetted for protection or not. Moreover, this analysis will allow individuals who want to write reproductive healthcare legislation to understand what arguments are more likely to work over others within certain political contexts.\n",
    "\n",
    "### ***How?***\n",
    "<font color=\"red\">400 words</font>\n",
    "\n",
    "## <font color=\"red\">*Pitch Your Sample*</font>\n",
    "\n",
    "<font color=\"red\">In the cell immediately following, describe the rationale behind your proposed sample design for your final project. What is the social game, social work, or social actors you about whom you are seeking to make inferences? What are its virtues with respect to your research questions? What are its limitations? What are alternatives? What would be a reasonable path to \"scale up\" your sample for further analysis (i.e., high-profile publication) beyond this class? (<300 words).\n",
    "\n",
    "### ***Which (words)?***\n",
    "<300 words</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ksvd\n",
      "  Downloading ksvd-0.0.3-py3-none-any.whl (3.0 kB)\n",
      "Requirement already satisfied: numpy in /opt/homebrew/lib/python3.11/site-packages (from ksvd) (1.26.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/homebrew/lib/python3.11/site-packages (from ksvd) (1.3.2)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->ksvd) (1.11.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->ksvd) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/homebrew/lib/python3.11/site-packages (from scikit-learn->ksvd) (3.2.0)\n",
      "Installing collected packages: ksvd\n",
      "Successfully installed ksvd-0.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Installations\n",
    "# %pip install -U git+https://github.com/UChicago-Computational-Content-Analysis/lucem_illud.git\n",
    "# %pip install fasttext\n",
    "# %pip install cython\n",
    "#%pip install ksvd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "\n",
    "# sklearn\n",
    "import sklearn.metrics.pairwise\n",
    "import sklearn.manifold\n",
    "import sklearn.decomposition\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# gensim\n",
    "import gensim\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.test.utils import datapath\n",
    "\n",
    "# models\n",
    "import fasttext\n",
    "from ksvd import ApproximateKSVD\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# misc\n",
    "import lucem_illud\n",
    "import re, string, cython, requests, nltk, copy, pickle, math\n",
    "from random import seed, sample\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the homework notebook\n",
    "def normalize(vector):\n",
    "    normalized_vector = vector / np.linalg.norm(vector)\n",
    "    return normalized_vector\n",
    "\n",
    "def dimension(model, positives, negatives):\n",
    "    diff = sum([normalize(model[x]) for x in positives]) - sum([normalize(model[y]) for y in negatives])\n",
    "    return diff\n",
    "\n",
    "# Plotting\n",
    "def Coloring(Series):\n",
    "    x = Series.values\n",
    "    y = x-x.min()\n",
    "    z = y/y.max()\n",
    "    c = list(plt.cm.rainbow(z))\n",
    "    return c\n",
    "\n",
    "def PlotDimension(ax,df, dim):\n",
    "    ax.set_frame_on(False)\n",
    "    ax.set_title(dim, fontsize = 20)\n",
    "    colors = Coloring(df[dim])\n",
    "    for i, word in enumerate(df.index):\n",
    "        ax.annotate(word, (0, df[dim][i]), color = colors[i], alpha = 0.6, fontsize = 12)\n",
    "    MaxY = df[dim].max()\n",
    "    MinY = df[dim].min()\n",
    "    plt.ylim(MinY,MaxY)\n",
    "    plt.yticks(())\n",
    "    plt.xticks(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_syn0norm(model):\n",
    "    \"\"\"since syn0norm is now depricated\"\"\"\n",
    "    return (model.wv.syn0 / np.sqrt((model.wv.syn0 ** 2).sum(-1))[..., np.newaxis]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smart_procrustes_align_gensim(base_embed, other_embed, gen, words=None):\n",
    "    base_embed = copy.copy(base_embed)\n",
    "    other_embed = copy.copy(other_embed)\n",
    "\n",
    "    # make sure vocabulary and indices are aligned\n",
    "    in_base_embed, in_other_embed = gen(base_embed, other_embed, words=words)\n",
    "\n",
    "    # get the embedding matrices\n",
    "    base_vecs= [in_base_embed.wv.get_vector(w,norm=True) for w in set(in_base_embed.wv.index_to_key)]\n",
    "    other_vecs= [in_other_embed.wv.get_vector(w,norm=True) for w in set(in_other_embed.wv.index_to_key)]\n",
    "\n",
    "    # just a matrix dot product with numpy\n",
    "    m = np.array(other_vecs).T.dot(np.array(base_vecs))\n",
    "\n",
    "    # SVD method from numpy\n",
    "    u, _, v = np.linalg.svd(m)\n",
    "\n",
    "    # another matrix operation\n",
    "    ortho = u.dot(v)\n",
    "    \n",
    "    # Replace original array with modified one\n",
    "    # i.e. multiplying the embedding matrix (syn0norm)by \"ortho\"\n",
    "    other_embed.wv.vectors =(np.array(other_vecs)).dot(ortho)\n",
    "    return other_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection_align_gensim(m1,m2, words=None):\n",
    "    \"\"\"\n",
    "    Intersect two gensim word2vec models, m1 and m2.\n",
    "    Only the shared vocabulary between them is kept.\n",
    "    If 'words' is set (as list or set), then the vocabulary is intersected with this list as well.\n",
    "    Indices are re-organized from 0..N in order of descending frequency (=sum of counts from both m1 and m2).\n",
    "    These indices correspond to the new syn0 and syn0norm objects in both gensim models:\n",
    "        -- so that Row 0 of m1.syn0 will be for the same word as Row 0 of m2.syn0\n",
    "        -- you can find the index of any word on the .index2word list: model.index2word.index(word) => 2\n",
    "    The .vocab dictionary is also updated for each model, preserving the count but updating the index.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the vocab for each model\n",
    "    vocab_m1 = set(m1.wv.index_to_key)\n",
    "    vocab_m2 = set(m2.wv.index_to_key)\n",
    "\n",
    "    # Find the common vocabulary\n",
    "    common_vocab = vocab_m1&vocab_m2\n",
    "    if words: common_vocab&=set(words)\n",
    "\n",
    "    # If no alignment necessary because vocab is identical...\n",
    "    if not vocab_m1-common_vocab and not vocab_m2-common_vocab:\n",
    "        return (m1,m2)\n",
    "\n",
    "    # Otherwise sort by frequency (summed for both)\n",
    "    common_vocab = list(common_vocab)\n",
    "    common_vocab.sort(key=lambda w: m1.wv.get_vecattr(w, \"count\")  + m2.wv.get_vecattr(w, \"count\") ,reverse=True)\n",
    "\n",
    "    # Then for each model...\n",
    "    for m in [m1,m2]:\n",
    "        # Replace old syn0norm array with new one (with common vocab)\n",
    "        new_arr = [m.wv.get_vector(w,norm=True) for w in common_vocab]\n",
    "\n",
    "        # Replace old vocab dictionary with new one (with common vocab)\n",
    "        # and old index2word with new one\n",
    "        m.index2word = common_vocab\n",
    "        # old_vocab = m.wv.index_to_key\n",
    "        new_vocab = []\n",
    "        k2i={}\n",
    "        for new_index,word in enumerate(common_vocab):\n",
    "            new_vocab.append(word)\n",
    "            k2i[word]=new_index\n",
    "        m.wv.index_to_key=new_vocab\n",
    "        m.wv.key_to_index=k2i\n",
    "        m.wv.vectors=np.array(new_arr)\n",
    "\n",
    "    return (m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareModels(df, raw_models, category, text_column_name='normalized_sents', sort = True, embeddings_raw={}):\n",
    "    \"\"\"If you are using time as your category sorting is important\"\"\"\n",
    "    if len(embeddings_raw) == 0:\n",
    "        embeddings_raw = raw_models(df, category, text_column_name, sort)\n",
    "    cats = sorted(set(df[category]))\n",
    "    #These are much quicker\n",
    "    embeddings_aligned = {}\n",
    "    for catOuter in cats:\n",
    "        embeddings_aligned[catOuter] = [embeddings_raw[catOuter]]\n",
    "        for catInner in cats:\n",
    "            embeddings_aligned[catOuter].append(smart_procrustes_align_gensim(embeddings_aligned[catOuter][-1], embeddings_raw[catInner]))\n",
    "    return embeddings_raw, embeddings_aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rawModels(df, category, text_column_name='normalized_sents', sort = True):\n",
    "    embeddings_raw = {}\n",
    "    cats = sorted(set(df[category]))\n",
    "    for cat in cats:\n",
    "        #This can take a while\n",
    "        print(\"Embedding {}\".format(cat), end = '\\r')\n",
    "        subsetDF = df[df[category] == cat]\n",
    "        #You might want to change the W2V parameters\n",
    "        embeddings_raw[cat] = gensim.models.word2vec.Word2Vec(subsetDF[text_column_name].sum())\n",
    "    return embeddings_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDivergenceDF(word, embeddingsDict):\n",
    "    dists = []\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "    dists = {}\n",
    "    print(word)\n",
    "    for cat in cats:\n",
    "        dists[cat] = []\n",
    "        for embed in embeddingsDict[cat][1:]:\n",
    "            dists[cat].append(np.abs(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cat][0].wv[word], axis = 0),\n",
    "                                                                             np.expand_dims(embed.wv[word], axis = 0))[0,0]))\n",
    "    return pd.DataFrame(dists, index = cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findDiverence(word, embeddingsDict):\n",
    "    cats = sorted(set(embeddingsDict.keys()))\n",
    "\n",
    "    dists = []\n",
    "    for embed in embeddingsDict[cats[0]][1:]:\n",
    "        try:\n",
    "            dists.append(1 - sklearn.metrics.pairwise.cosine_similarity(np.expand_dims(embeddingsDict[cats[0]][0].wv[word], axis = 0), np.expand_dims(embed.wv[word], axis = 0))[0,0])\n",
    "        except:\n",
    "            pass\n",
    "    return np.mean(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMostDivergent(embeddingsDict, comparedEmbeddings):\n",
    "    original_words = comparedEmbeddings[1950][0].wv.index_to_key\n",
    "    for embeds in embeddingsDict.values():\n",
    "        for embed in embeds:\n",
    "            original_words = set(original_words).intersection(set(embed.wv.index_to_key))\n",
    "    words = set(original_words)\n",
    "    print(\"Found {} words to compare\".format(len(words)))\n",
    "    return sorted([(w, findDiverence(w, embeddingsDict)) for w in words], key = lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_to_embeddings(address, kind):\n",
    "    rawEmbeddings = {}\n",
    "    for file in os.listdir(address):\n",
    "        if \"embedding_\"+kind in file:\n",
    "            e, kind_, kind_type = file.split(\"_\")\n",
    "            kind_type = eval(kind_type)\n",
    "            rawEmbeddings[kind_type] = Word2Vec.load(file)\n",
    "    return rawEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get random indices\n",
    "def random_indices(df_len, num_indices):\n",
    "    \"\"\"\n",
    "    Generate a list of random indices.\n",
    "\n",
    "    returns: list of random indices\n",
    "    \"\"\"\n",
    "    random_indices = np.random.choice(df_len, num_indices, replace=False)\n",
    "    random_indices = list(random_indices)\n",
    "    random_indices.sort()\n",
    "\n",
    "    return random_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>congress_num</th>\n",
       "      <th>legislation number</th>\n",
       "      <th>title</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118</td>\n",
       "      <td>H.R. 2907</td>\n",
       "      <td>Let Doctors Provide Reproductive Health Care Act</td>\n",
       "      <td>Congressional Bills 118th Congress From the U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118</td>\n",
       "      <td>S. 1297</td>\n",
       "      <td>Let Doctors Provide Reproductive Health Care Act</td>\n",
       "      <td>Congressional Bills 118th Congress From the U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118</td>\n",
       "      <td>H.R. 4901</td>\n",
       "      <td>Reproductive Health Care Accessibility Act</td>\n",
       "      <td>Congressional Bills 118th Congress From the U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>118</td>\n",
       "      <td>S. 2544</td>\n",
       "      <td>Reproductive Health Care Accessibility Act</td>\n",
       "      <td>Congressional Bills 118th Congress From the U....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118</td>\n",
       "      <td>H.R. 4147</td>\n",
       "      <td>Reproductive Health Care Training Act of 2023</td>\n",
       "      <td>Congressional Bills 118th Congress From the U....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   congress_num legislation number  \\\n",
       "0           118          H.R. 2907   \n",
       "1           118            S. 1297   \n",
       "2           118          H.R. 4901   \n",
       "3           118            S. 2544   \n",
       "4           118          H.R. 4147   \n",
       "\n",
       "                                              title  \\\n",
       "0  Let Doctors Provide Reproductive Health Care Act   \n",
       "1  Let Doctors Provide Reproductive Health Care Act   \n",
       "2        Reproductive Health Care Accessibility Act   \n",
       "3        Reproductive Health Care Accessibility Act   \n",
       "4     Reproductive Health Care Training Act of 2023   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  Congressional Bills 118th Congress From the U....  \n",
       "1  Congressional Bills 118th Congress From the U....  \n",
       "2  Congressional Bills 118th Congress From the U....  \n",
       "3  Congressional Bills 118th Congress From the U....  \n",
       "4  Congressional Bills 118th Congress From the U....  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading congressional data\n",
    "congress_df = pd.read_csv('../data/congress_legislation_cleaned.csv')\n",
    "congress_df = congress_df.loc[:, ['congress_num', 'legislation number', 'title', 'cleaned_text']]\n",
    "congress_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>year</th>\n",
       "      <th>author</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Dobbs v. Jackson Women's Health Organization</td>\n",
       "      <td>2022</td>\n",
       "      <td>Samuel A. Alito, Jr.</td>\n",
       "      <td>1 (Slip Opinion) OCTOBER TERM, 2021 Syllabus N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Whole Woman's Health v. Hellerstedt</td>\n",
       "      <td>2016</td>\n",
       "      <td>Stephen Breyer</td>\n",
       "      <td>1 (Slip Opinion) OCTOBER TERM, 2015 Syllabus N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gonzales v. Carhart</td>\n",
       "      <td>2007</td>\n",
       "      <td>Anthony Kennedy</td>\n",
       "      <td>550US1 U nit: U31 07 28 10 12:14:15 P A GES PG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stenberg v. Carhart</td>\n",
       "      <td>2000</td>\n",
       "      <td>Stephen Breyer</td>\n",
       "      <td>OCTOBER TERM, 1999 Syllabus STENBERG, ATTORNEY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Planned Parenthood of Southeastern Pennsylvani...</td>\n",
       "      <td>1992</td>\n",
       "      <td>Anthony Kennedy, David Souter, Sandra Day O’Co...</td>\n",
       "      <td>505us3u117 07 09 96 09:34:02 PAGES OPINPGT 833...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                case  year  \\\n",
       "0       Dobbs v. Jackson Women's Health Organization  2022   \n",
       "1                Whole Woman's Health v. Hellerstedt  2016   \n",
       "2                                Gonzales v. Carhart  2007   \n",
       "3                                Stenberg v. Carhart  2000   \n",
       "4  Planned Parenthood of Southeastern Pennsylvani...  1992   \n",
       "\n",
       "                                              author  \\\n",
       "0                               Samuel A. Alito, Jr.   \n",
       "1                                     Stephen Breyer   \n",
       "2                                    Anthony Kennedy   \n",
       "3                                     Stephen Breyer   \n",
       "4  Anthony Kennedy, David Souter, Sandra Day O’Co...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  1 (Slip Opinion) OCTOBER TERM, 2021 Syllabus N...  \n",
       "1  1 (Slip Opinion) OCTOBER TERM, 2015 Syllabus N...  \n",
       "2  550US1 U nit: U31 07 28 10 12:14:15 P A GES PG...  \n",
       "3  OCTOBER TERM, 1999 Syllabus STENBERG, ATTORNEY...  \n",
       "4  505us3u117 07 09 96 09:34:02 PAGES OPINPGT 833...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading scotus data\n",
    "scotus_df = pd.read_csv('../data/scotus_cases_cleaned.csv')\n",
    "scotus_df = scotus_df.loc[:, ['case', 'year', 'author', 'cleaned_text']]\n",
    "scotus_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1243/1243 [2:33:00<00:00,  7.39s/it]   \n"
     ]
    }
   ],
   "source": [
    "# tokenize and normalize sentences\n",
    "# congress_df['tokenized_sents'] = congress_df['cleaned_text'].progress_apply(lambda x: [lucem_illud.word_tokenize(s, MAX_LEN=5000000) for s in lucem_illud.sent_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "# congress_df['normalized_sents'] = congress_df['tokenized_sents'].apply(lambda x: [lucem_illud.normalizeTokens(s, lemma=False) for s in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_df = pd.read_csv('../data/congress_legislation_cleaned_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_df.to_csv(\"congress_legislation_tokenized_sents.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">*Exercise 1*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project using at least two different specification of `word2vec` and/or `fasttext`, and visualize them each with two separate visualization layout specifications (e.g., TSNE, PCA). Then interrogate critical word vectors within your corpus in terms of the most similar words, analogies, and other additions and subtractions that reveal the structure of similarity and difference within your semantic space. What does this pattern reveal about the semantic organization of words in your corpora? Which estimation and visualization specification generate the most insight and appear the most robustly supported and why?\n",
    "\n",
    "<font color=\"red\">***Stretch***: Explore different vector calculations beyond addition and subtraction, such as multiplication, division or some other function. What does this exploration reveal about the semantic structure of your corpus?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"abortion\" vector:  [-4.8669734  -0.6166447   1.3110815   0.72449046  0.7318664  -0.4321819\n",
      " -6.2501717   1.7904253   1.7648962  -1.5254838 ]\n",
      "word at index 10:  united\n"
     ]
    }
   ],
   "source": [
    "congress_w2v = gensim.models.word2vec.Word2Vec(congress_df['normalized_sents'].sum(), sg=0)\n",
    "print('\"abortion\" vector: ', congress_w2v.wv['abortion'][:10])\n",
    "print('word at index 10: ', congress_w2v.wv.index_to_key[10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word2Vec Similarity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abortions', 0.7530140280723572),\n",
       " ('sterilization', 0.5249667763710022),\n",
       " ('induced', 0.5214768052101135),\n",
       " ('counsels', 0.517237663269043),\n",
       " ('obstetric', 0.4906849265098572),\n",
       " ('overt', 0.4834439754486084),\n",
       " ('incest', 0.45907044410705566),\n",
       " ('mother', 0.4565138518810272),\n",
       " ('heartbeat', 0.443374365568161),\n",
       " ('unemancipated', 0.43996715545654297)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to abortion\n",
    "congress_w2v.wv.most_similar('abortion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('safeguards', 0.622541606426239),\n",
       " ('disclosure', 0.5984277725219727),\n",
       " ('confidentiality', 0.5983681082725525),\n",
       " ('liberties', 0.5723415017127991),\n",
       " ('identity', 0.5707226991653442),\n",
       " ('protects', 0.5446917414665222),\n",
       " ('retaliation', 0.537446141242981),\n",
       " ('whistleblowers', 0.5234901905059814),\n",
       " ('protections', 0.5112161040306091),\n",
       " ('safeguard', 0.49264729022979736)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to privacy\n",
    "congress_w2v.wv.most_similar('privacy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('protects', 0.6279311180114746),\n",
       " ('preserve', 0.6121134161949158),\n",
       " ('safeguard', 0.6113435626029968),\n",
       " ('protecting', 0.5907005667686462),\n",
       " ('assure', 0.5389564633369446),\n",
       " ('desirable', 0.5373063087463379),\n",
       " ('avert', 0.5347046256065369),\n",
       " ('precautions', 0.5292848348617554),\n",
       " ('prevent', 0.5214537382125854),\n",
       " ('ensuring', 0.508388340473175)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('protect')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mother', 0.7277065515518188),\n",
       " ('vaginally', 0.7140676975250244),\n",
       " ('kills', 0.7091748118400574),\n",
       " ('conception', 0.7035084962844849),\n",
       " ('heartbeat', 0.6972429156303406),\n",
       " ('fertilization', 0.690980076789856),\n",
       " ('detectable', 0.6764907240867615),\n",
       " ('embryonic', 0.673604428768158),\n",
       " ('overt', 0.6729521155357361),\n",
       " ('infanticide', 0.6714892983436584)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('fetus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('infant', 0.7119036316871643),\n",
       " ('clothing', 0.622071385383606),\n",
       " ('perinatal', 0.6124640107154846),\n",
       " ('maternity', 0.599256694316864),\n",
       " ('diapers', 0.5970709919929504),\n",
       " ('childcare', 0.5838642716407776),\n",
       " ('infants', 0.5803161859512329),\n",
       " ('postnatal', 0.5798137784004211),\n",
       " ('male', 0.5770944356918335),\n",
       " ('otc', 0.5748292803764343)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('baby')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('depends', 0.6447806358337402),\n",
       " ('heartbeat', 0.6290205121040344),\n",
       " ('harmful', 0.6250562071800232),\n",
       " ('cortex', 0.6246358156204224),\n",
       " ('connections', 0.6210643649101257),\n",
       " ('perception', 0.6148685216903687),\n",
       " ('rests', 0.6061360239982605),\n",
       " ('obstacle', 0.5973332524299622),\n",
       " ('thalamus', 0.591398298740387),\n",
       " ('react', 0.5808981657028198)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('viability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('helps', 0.6987822651863098),\n",
       " ('breastfeeding', 0.6119071841239929),\n",
       " ('abstinence', 0.6102300882339478),\n",
       " ('adulthood', 0.6010406613349915),\n",
       " ('culturally', 0.5891531109809875),\n",
       " ('literacy', 0.5780025720596313),\n",
       " ('preschool', 0.5665664076805115),\n",
       " ('paraprofessionals', 0.566216230392456),\n",
       " ('adolescent', 0.5647426247596741),\n",
       " ('stem', 0.5641573071479797)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('sexuality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('infertility', 0.7163260579109192),\n",
       " ('infections', 0.6544108986854553),\n",
       " ('childbirth', 0.6419183611869812),\n",
       " ('postpartum', 0.6369994878768921),\n",
       " ('infection', 0.6148290038108826),\n",
       " ('prenatal', 0.614122211933136),\n",
       " ('interventions', 0.6081894040107727),\n",
       " ('motherhood', 0.5975733995437622),\n",
       " ('breastfeeding', 0.5904054045677185),\n",
       " ('intervention', 0.5898716449737549)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine words most similar to protect\n",
    "congress_w2v.wv.most_similar('contraception')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'healthcare'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find words most disimilar to [INSERT WORD HERE]\n",
    "congress_w2v.wv.doesnt_match(['abortion', 'healthcare', 'right', 'choice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use semantic equations to find words most analogous to [INSERT ANALOGY HERE]\n",
    "# equation: X + Y - Z = ___ (X is to Z as Y is to ___)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "numWords = 50\n",
    "indices = random_indices(len(congress_w2v.wv.index_to_key), numWords)\n",
    "targetWords = congress_w2v.wv.index_to_key[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">*Exercise 2*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project using `doc2vec`, and explore the relationship between different documents and the word vectors you analyzed in the last exercise. Consider the most similar words to critical documents, analogies (doc _x_ + word _y_), and other additions and subtractions that reveal the structure of similarity and difference within your semantic space. What does this pattern reveal about the documentary organization of your semantic space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">*Exercise 3*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that embed documents related to your final project, then generate meaningful semantic dimensions based on your theoretical understanding of the semantic space (i.e., by subtracting semantically opposite word vectors) and project another set of word vectors onto those dimensions. Interpret the meaning of these projections for your analysis. Which of the dimensions you analyze explain the most variation in the projection of your words and why?\n",
    "\n",
    "<font color=\"red\">***Stretch***: Average together multiple antonym pairs to create robust semantic dimensions. How do word projections on these robust dimensions differ from single-pair dimensions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">*Exercise 4*</font>\n",
    "\n",
    "<font color=\"red\">Construct cells immediately below this that align word embeddings over time or across domains/corpora. Interrogate the spaces that result and ask which words changed most and least over the entire period or between contexts/corpora. What does this reveal about the social game underlying your space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
